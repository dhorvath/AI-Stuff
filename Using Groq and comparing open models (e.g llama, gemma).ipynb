{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNfYZwPcWuXegQXcDB/K6nT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhorvath/AI-Stuff/blob/main/Using%20Groq%20and%20comparing%20open%20models%20(e.g%20llama%2C%20gemma).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "lzrOmSVNPKzs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "csLBY4riNdsU",
        "outputId": "5989f6be-31e6-4eba-a78c-682d42374b8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index\n",
            "  Downloading llama_index-0.11.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (2.8.2)\n",
            "Collecting llama-index-agent-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_agent_openai-0.3.0-py3-none-any.whl.metadata (728 bytes)\n",
            "Collecting llama-index-cli<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_cli-0.3.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting llama-index-core==0.11.0.post1 (from llama-index)\n",
            "  Downloading llama_index_core-0.11.0.post1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting llama-index-embeddings-openai<0.3.0,>=0.2.0 (from llama-index)\n",
            "  Downloading llama_index_embeddings_openai-0.2.3-py3-none-any.whl.metadata (635 bytes)\n",
            "Collecting llama-index-indices-managed-llama-cloud>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.3.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index)\n",
            "  Downloading llama_index_legacy-0.9.48.post3-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting llama-index-llms-openai<0.3.0,>=0.2.0 (from llama-index)\n",
            "  Downloading llama_index_llms_openai-0.2.0-py3-none-any.whl.metadata (648 bytes)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.3.0,>=0.2.0 (from llama-index)\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.2.0-py3-none-any.whl.metadata (728 bytes)\n",
            "Collecting llama-index-program-openai<0.3.0,>=0.2.0 (from llama-index)\n",
            "  Downloading llama_index_program_openai-0.2.0-py3-none-any.whl.metadata (766 bytes)\n",
            "Collecting llama-index-question-gen-openai<0.3.0,>=0.2.0 (from llama-index)\n",
            "  Downloading llama_index_question_gen_openai-0.2.0-py3-none-any.whl.metadata (785 bytes)\n",
            "Collecting llama-index-readers-file<0.3.0,>=0.2.0 (from llama-index)\n",
            "  Downloading llama_index_readers_file-0.2.0-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting llama-index-readers-llama-parse>=0.2.0 (from llama-index)\n",
            "  Downloading llama_index_readers_llama_parse-0.2.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting nltk>3.8.1 (from llama-index)\n",
            "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.11.0.post1->llama-index) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core==0.11.0.post1->llama-index) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.11.0.post1->llama-index) (3.10.5)\n",
            "Collecting dataclasses-json (from llama-index-core==0.11.0.post1->llama-index)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting deprecated>=1.2.9.3 (from llama-index-core==0.11.0.post1->llama-index)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core==0.11.0.post1->llama-index)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.11.0.post1->llama-index) (2024.6.1)\n",
            "Collecting httpx (from llama-index-core==0.11.0.post1->llama-index)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.11.0.post1->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.11.0.post1->llama-index) (3.3)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.11.0.post1->llama-index) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.11.0.post1->llama-index) (9.4.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.11.0.post1->llama-index) (2.32.3)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.2.0 (from llama-index-core==0.11.0.post1->llama-index)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index-core==0.11.0.post1->llama-index)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.11.0.post1->llama-index) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.11.0.post1->llama-index) (4.12.2)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index-core==0.11.0.post1->llama-index)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.11.0.post1->llama-index) (1.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic) (2.20.1)\n",
            "Collecting openai>=1.14.0 (from llama-index-agent-openai<0.4.0,>=0.3.0->llama-index)\n",
            "  Downloading openai-1.42.0-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting llama-cloud>=0.0.11 (from llama-index-indices-managed-llama-cloud>=0.3.0->llama-index)\n",
            "  Downloading llama_cloud-0.0.15-py3-none-any.whl.metadata (751 bytes)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.1.4)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index) (4.12.3)\n",
            "Collecting pypdf<5.0.0,>=4.0.1 (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index)\n",
            "  Downloading pypdf-4.3.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index)\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse>=0.2.0->llama-index)\n",
            "  Downloading llama_parse-0.5.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index) (2024.5.15)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.11.0.post1->llama-index) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.11.0.post1->llama-index) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.11.0.post1->llama-index) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.11.0.post1->llama-index) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.11.0.post1->llama-index) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.11.0.post1->llama-index) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.11.0.post1->llama-index) (4.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.3.0,>=0.2.0->llama-index) (2.6)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core==0.11.0.post1->llama-index) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core==0.11.0.post1->llama-index) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx->llama-index-core==0.11.0.post1->llama-index)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core==0.11.0.post1->llama-index) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core==0.11.0.post1->llama-index) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->llama-index-core==0.11.0.post1->llama-index)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.0->llama-index) (1.7.0)\n",
            "Collecting jiter<1,>=0.4.0 (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.0->llama-index)\n",
            "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core==0.11.0.post1->llama-index) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core==0.11.0.post1->llama-index) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core==0.11.0.post1->llama-index) (3.0.3)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core==0.11.0.post1->llama-index)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core==0.11.0.post1->llama-index)\n",
            "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2024.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core==0.11.0.post1->llama-index) (1.2.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core==0.11.0.post1->llama-index) (24.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (1.16.0)\n",
            "Downloading llama_index-0.11.0-py3-none-any.whl (6.8 kB)\n",
            "Downloading llama_index_core-0.11.0.post1-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_agent_openai-0.3.0-py3-none-any.whl (13 kB)\n",
            "Downloading llama_index_cli-0.3.0-py3-none-any.whl (27 kB)\n",
            "Downloading llama_index_embeddings_openai-0.2.3-py3-none-any.whl (6.3 kB)\n",
            "Downloading llama_index_indices_managed_llama_cloud-0.3.0-py3-none-any.whl (9.5 kB)\n",
            "Downloading llama_index_legacy-0.9.48.post3-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_llms_openai-0.2.0-py3-none-any.whl (12 kB)\n",
            "Downloading llama_index_multi_modal_llms_openai-0.2.0-py3-none-any.whl (5.9 kB)\n",
            "Downloading llama_index_program_openai-0.2.0-py3-none-any.whl (5.3 kB)\n",
            "Downloading llama_index_question_gen_openai-0.2.0-py3-none-any.whl (2.9 kB)\n",
            "Downloading llama_index_readers_file-0.2.0-py3-none-any.whl (38 kB)\n",
            "Downloading llama_index_readers_llama_parse-0.2.0-py3-none-any.whl (2.5 kB)\n",
            "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Downloading llama_cloud-0.0.15-py3-none-any.whl (180 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.2/180.2 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_parse-0.5.0-py3-none-any.whl (9.4 kB)\n",
            "Downloading openai-1.42.0-py3-none-any.whl (362 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.9/362.9 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-4.3.1-py3-none-any.whl (295 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Downloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: striprtf, dirtyjson, tenacity, pypdf, nltk, mypy-extensions, marshmallow, jiter, h11, deprecated, typing-inspect, tiktoken, httpcore, httpx, dataclasses-json, openai, llama-index-core, llama-cloud, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-legacy, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.8.1\n",
            "    Uninstalling nltk-3.8.1:\n",
            "      Successfully uninstalled nltk-3.8.1\n",
            "Successfully installed dataclasses-json-0.6.7 deprecated-1.2.14 dirtyjson-1.0.8 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 jiter-0.5.0 llama-cloud-0.0.15 llama-index-0.11.0 llama-index-agent-openai-0.3.0 llama-index-cli-0.3.0 llama-index-core-0.11.0.post1 llama-index-embeddings-openai-0.2.3 llama-index-indices-managed-llama-cloud-0.3.0 llama-index-legacy-0.9.48.post3 llama-index-llms-openai-0.2.0 llama-index-multi-modal-llms-openai-0.2.0 llama-index-program-openai-0.2.0 llama-index-question-gen-openai-0.2.0 llama-index-readers-file-0.2.0 llama-index-readers-llama-parse-0.2.0 llama-parse-0.5.0 marshmallow-3.22.0 mypy-extensions-1.0.0 nltk-3.9.1 openai-1.42.0 pypdf-4.3.1 striprtf-0.0.26 tenacity-8.5.0 tiktoken-0.7.0 typing-inspect-0.9.0\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (2.8.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic) (2.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic) (4.12.2)\n",
            "Collecting llama-index-llms-groq\n",
            "  Downloading llama_index_llms_groq-0.2.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.42.0)\n",
            "Requirement already satisfied: llama-index-core<0.12.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-groq) (0.11.0.post1)\n",
            "Collecting llama-index-llms-openai-like<0.3.0,>=0.2.0 (from llama-index-llms-groq)\n",
            "  Downloading llama_index_llms_openai_like-0.2.0-py3-none-any.whl.metadata (753 bytes)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.5.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (3.10.5)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (1.0.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (2024.6.1)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (3.3)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (3.9.1)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (9.4.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (0.7.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (1.16.0)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-openai-like<0.3.0,>=0.2.0->llama-index-llms-groq) (0.2.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.37.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-openai-like<0.3.0,>=0.2.0->llama-index-llms-groq) (4.42.4)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (4.0.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (2024.5.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (3.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.37.0->llama-index-llms-openai-like<0.3.0,>=0.2.0->llama-index-llms-groq) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.37.0->llama-index-llms-openai-like<0.3.0,>=0.2.0->llama-index-llms-groq) (0.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.37.0->llama-index-llms-openai-like<0.3.0,>=0.2.0->llama-index-llms-groq) (24.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.37.0->llama-index-llms-openai-like<0.3.0,>=0.2.0->llama-index-llms-groq) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.37.0->llama-index-llms-openai-like<0.3.0,>=0.2.0->llama-index-llms-groq) (0.19.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (3.22.0)\n",
            "Downloading llama_index_llms_groq-0.2.0-py3-none-any.whl (2.9 kB)\n",
            "Downloading llama_index_llms_openai_like-0.2.0-py3-none-any.whl (3.1 kB)\n",
            "Installing collected packages: llama-index-llms-openai-like, llama-index-llms-groq\n",
            "Successfully installed llama-index-llms-groq-0.2.0 llama-index-llms-openai-like-0.2.0\n",
            "Collecting textwrap3\n",
            "  Downloading textwrap3-0.9.2-py2.py3-none-any.whl.metadata (4.6 kB)\n",
            "Downloading textwrap3-0.9.2-py2.py3-none-any.whl (12 kB)\n",
            "Installing collected packages: textwrap3\n",
            "Successfully installed textwrap3-0.9.2\n"
          ]
        }
      ],
      "source": [
        "# Installs\n",
        "!pip install --upgrade llama-index pydantic\n",
        "!pip install --upgrade pydantic\n",
        "!pip install llama-index-llms-groq openai\n",
        "!pip install textwrap3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "from google.colab import userdata\n",
        "from google.colab import files\n",
        "import os"
      ],
      "metadata": {
        "id": "3A3RwMNxOow2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "groq_api_key = userdata.get('groq_api_key')\n",
        "open_ai_key = userdata.get('open_ai_key')"
      ],
      "metadata": {
        "id": "zrJw35LNWqaC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = open_ai_key\n",
        "# os.environ[\"OPENAI_MODEL_NAME\"]=\"gpt-4o-mini\""
      ],
      "metadata": {
        "id": "h10ucmUrcDvt"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index-llms-groq\n",
        "!pip install llama-index"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOGZA4XuX6Se",
        "outputId": "2d97f3b7-e378-4471-8065-ea9b4439ae63"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llama-index-llms-groq in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Requirement already satisfied: llama-index-core<0.12.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-groq) (0.11.0.post1)\n",
            "Requirement already satisfied: llama-index-llms-openai-like<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-groq) (0.2.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (3.10.5)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (1.0.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (2024.6.1)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (0.27.0)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (3.3)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (3.9.1)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (9.4.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (2.8.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (0.7.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (1.16.0)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-openai-like<0.3.0,>=0.2.0->llama-index-llms-groq) (0.2.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.37.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-openai-like<0.3.0,>=0.2.0->llama-index-llms-groq) (4.42.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (4.0.3)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.40.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-openai<0.3.0,>=0.2.0->llama-index-llms-openai-like<0.3.0,>=0.2.0->llama-index-llms-groq) (1.42.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (2024.5.15)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (2024.7.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (3.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.37.0->llama-index-llms-openai-like<0.3.0,>=0.2.0->llama-index-llms-groq) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.37.0->llama-index-llms-openai-like<0.3.0,>=0.2.0->llama-index-llms-groq) (0.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.37.0->llama-index-llms-openai-like<0.3.0,>=0.2.0->llama-index-llms-groq) (24.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.37.0->llama-index-llms-openai-like<0.3.0,>=0.2.0->llama-index-llms-groq) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.37.0->llama-index-llms-openai-like<0.3.0,>=0.2.0->llama-index-llms-groq) (0.19.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (3.22.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (0.14.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.40.0->llama-index-llms-openai<0.3.0,>=0.2.0->llama-index-llms-openai-like<0.3.0,>=0.2.0->llama-index-llms-groq) (1.7.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->llama-index-llms-openai<0.3.0,>=0.2.0->llama-index-llms-openai-like<0.3.0,>=0.2.0->llama-index-llms-groq) (0.5.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (1.2.2)\n",
            "Requirement already satisfied: llama-index in /usr/local/lib/python3.10/dist-packages (0.11.0)\n",
            "Requirement already satisfied: llama-index-agent-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.3.0)\n",
            "Requirement already satisfied: llama-index-cli<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.3.0)\n",
            "Requirement already satisfied: llama-index-core==0.11.0.post1 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.11.0.post1)\n",
            "Requirement already satisfied: llama-index-embeddings-openai<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.2.3)\n",
            "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.3.0)\n",
            "Requirement already satisfied: llama-index-legacy<0.10.0,>=0.9.48 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.9.48.post3)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.2.0)\n",
            "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.2.0)\n",
            "Requirement already satisfied: llama-index-program-openai<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.2.0)\n",
            "Requirement already satisfied: llama-index-question-gen-openai<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.2.0)\n",
            "Requirement already satisfied: llama-index-readers-file<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.2.0)\n",
            "Requirement already satisfied: llama-index-readers-llama-parse>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.2.0)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index) (3.9.1)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.11.0.post1->llama-index) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core==0.11.0.post1->llama-index) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.11.0.post1->llama-index) (3.10.5)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.11.0.post1->llama-index) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.11.0.post1->llama-index) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.11.0.post1->llama-index) (1.0.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.11.0.post1->llama-index) (2024.6.1)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.11.0.post1->llama-index) (0.27.0)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.11.0.post1->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.11.0.post1->llama-index) (3.3)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.11.0.post1->llama-index) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.11.0.post1->llama-index) (9.4.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.11.0.post1->llama-index) (2.8.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.11.0.post1->llama-index) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.11.0.post1->llama-index) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.11.0.post1->llama-index) (0.7.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.11.0.post1->llama-index) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.11.0.post1->llama-index) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.11.0.post1->llama-index) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.11.0.post1->llama-index) (1.16.0)\n",
            "Requirement already satisfied: openai>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-agent-openai<0.4.0,>=0.3.0->llama-index) (1.42.0)\n",
            "Requirement already satisfied: llama-cloud>=0.0.11 in /usr/local/lib/python3.10/dist-packages (from llama-index-indices-managed-llama-cloud>=0.3.0->llama-index) (0.0.15)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.1.4)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index) (4.12.3)\n",
            "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index) (4.3.1)\n",
            "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index) (0.0.26)\n",
            "Requirement already satisfied: llama-parse>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-llama-parse>=0.2.0->llama-index) (0.5.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index) (2024.5.15)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.11.0.post1->llama-index) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.11.0.post1->llama-index) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.11.0.post1->llama-index) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.11.0.post1->llama-index) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.11.0.post1->llama-index) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.11.0.post1->llama-index) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.11.0.post1->llama-index) (4.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.3.0,>=0.2.0->llama-index) (2.6)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core==0.11.0.post1->llama-index) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core==0.11.0.post1->llama-index) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core==0.11.0.post1->llama-index) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core==0.11.0.post1->llama-index) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core==0.11.0.post1->llama-index) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core==0.11.0.post1->llama-index) (0.14.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.0->llama-index) (1.7.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.0->llama-index) (0.5.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->llama-index-core==0.11.0.post1->llama-index) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->llama-index-core==0.11.0.post1->llama-index) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core==0.11.0.post1->llama-index) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core==0.11.0.post1->llama-index) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core==0.11.0.post1->llama-index) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core==0.11.0.post1->llama-index) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core==0.11.0.post1->llama-index) (3.22.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2024.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core==0.11.0.post1->llama-index) (1.2.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core==0.11.0.post1->llama-index) (24.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper\n",
        "import textwrap3\n",
        "def wrap_print(long_text):\n",
        "  print('\\n'.join(textwrap3.wrap(long_text)))"
      ],
      "metadata": {
        "id": "A33TD1YGPDMQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data"
      ],
      "metadata": {
        "id": "ZBg-UB4iPIWu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def upload_files():\n",
        "    uploaded = files.upload()  # Now this refers to the correct module\n",
        "    for filename, content in uploaded.items():\n",
        "        with open(filename, 'wb') as f:\n",
        "            f.write(content)\n",
        "        print(f'Uploaded {filename}')\n",
        "\n",
        "upload_files()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tT8AR3CzV4u3",
        "outputId": "2e4dbd0b-da33-4a9f-d393-a36c95b4e9ae"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-49e89667-e653-4acd-a48a-00f099e2836c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-49e89667-e653-4acd-a48a-00f099e2836c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Suspect Guide - Eat, Pray, Die!.docx to Suspect Guide - Eat, Pray, Die!.docx\n",
            "Saving 1. Announcement of Murder (pt. I).docx to 1. Announcement of Murder (pt. I).docx\n",
            "Saving 2. Announcement of Murder (pt. II).docx to 2. Announcement of Murder (pt. II).docx\n",
            "Saving Ty Park.docx to Ty Park.docx\n",
            "Saving Travis Astor-Malone_.docx to Travis Astor-Malone_.docx\n",
            "Saving Sophia Astor.docx to Sophia Astor.docx\n",
            "Saving Sloane Sharpe.docx to Sloane Sharpe.docx\n",
            "Saving Sandra Grayson.docx to Sandra Grayson.docx\n",
            "Saving Samuel Remington.docx to Samuel Remington.docx\n",
            "Saving Princess Margaret Amelia.docx to Princess Margaret Amelia.docx\n",
            "Saving Oliver Aldridge.docx to Oliver Aldridge.docx\n",
            "Saving Miss Meera.docx to Miss Meera.docx\n",
            "Saving Jamie Blackwood.docx to Jamie Blackwood.docx\n",
            "Saving Harper Hayes.docx to Harper Hayes.docx\n",
            "Saving Duke Tad Rees.docx to Duke Tad Rees.docx\n",
            "Saving Crystal Powers.docx to Crystal Powers.docx\n",
            "Saving Colt Remington.docx to Colt Remington.docx\n",
            "Saving Ty Park (pt. II).docx to Ty Park (pt. II).docx\n",
            "Saving Travis Astor-Malone (pt. II).docx to Travis Astor-Malone (pt. II).docx\n",
            "Saving Sophia Astor (pt. II).docx to Sophia Astor (pt. II).docx\n",
            "Saving Sloane Sharpe (pt. II).docx to Sloane Sharpe (pt. II).docx\n",
            "Saving Sandra Grayson (pt. II).docx to Sandra Grayson (pt. II).docx\n",
            "Saving Samuel Remington (pt. II).docx to Samuel Remington (pt. II).docx\n",
            "Saving Princess Margaret Amelia (pt. II).docx to Princess Margaret Amelia (pt. II).docx\n",
            "Saving Oliver Aldridge (pt. II).docx to Oliver Aldridge (pt. II).docx\n",
            "Saving Miss Meera (pt. II).docx to Miss Meera (pt. II).docx\n",
            "Saving Jamie Blackwood (pt. II).docx to Jamie Blackwood (pt. II).docx\n",
            "Saving Harper Hayes (pt. II).docx to Harper Hayes (pt. II).docx\n",
            "Saving Duke Tad Rees (pt. II).docx to Duke Tad Rees (pt. II).docx\n",
            "Saving Crystal Powers (pt.II).docx to Crystal Powers (pt.II).docx\n",
            "Saving Colt Remington (pt. II).docx to Colt Remington (pt. II).docx\n",
            "Saving Serenity Springs Map to Serenity Springs Map\n",
            "Saving Accusation form.docx to Accusation form.docx\n",
            "Uploaded Suspect Guide - Eat, Pray, Die!.docx\n",
            "Uploaded 1. Announcement of Murder (pt. I).docx\n",
            "Uploaded 2. Announcement of Murder (pt. II).docx\n",
            "Uploaded Ty Park.docx\n",
            "Uploaded Travis Astor-Malone_.docx\n",
            "Uploaded Sophia Astor.docx\n",
            "Uploaded Sloane Sharpe.docx\n",
            "Uploaded Sandra Grayson.docx\n",
            "Uploaded Samuel Remington.docx\n",
            "Uploaded Princess Margaret Amelia.docx\n",
            "Uploaded Oliver Aldridge.docx\n",
            "Uploaded Miss Meera.docx\n",
            "Uploaded Jamie Blackwood.docx\n",
            "Uploaded Harper Hayes.docx\n",
            "Uploaded Duke Tad Rees.docx\n",
            "Uploaded Crystal Powers.docx\n",
            "Uploaded Colt Remington.docx\n",
            "Uploaded Ty Park (pt. II).docx\n",
            "Uploaded Travis Astor-Malone (pt. II).docx\n",
            "Uploaded Sophia Astor (pt. II).docx\n",
            "Uploaded Sloane Sharpe (pt. II).docx\n",
            "Uploaded Sandra Grayson (pt. II).docx\n",
            "Uploaded Samuel Remington (pt. II).docx\n",
            "Uploaded Princess Margaret Amelia (pt. II).docx\n",
            "Uploaded Oliver Aldridge (pt. II).docx\n",
            "Uploaded Miss Meera (pt. II).docx\n",
            "Uploaded Jamie Blackwood (pt. II).docx\n",
            "Uploaded Harper Hayes (pt. II).docx\n",
            "Uploaded Duke Tad Rees (pt. II).docx\n",
            "Uploaded Crystal Powers (pt.II).docx\n",
            "Uploaded Colt Remington (pt. II).docx\n",
            "Uploaded Serenity Springs Map\n",
            "Uploaded Accusation form.docx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kbnn9qJAZNSl",
        "outputId": "6fcbbd6a-bb87-4c1a-91eb-d9d23afdd198"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'1. Announcement of Murder (pt. I).docx'   'Princess Margaret Amelia.docx'\n",
            "'2. Announcement of Murder (pt. II).docx'  'Princess Margaret Amelia (pt. II).docx'\n",
            "'Accusation form.docx'\t\t\t    sample_data\n",
            "'Colt Remington.docx'\t\t\t   'Samuel Remington.docx'\n",
            "'Colt Remington (pt. II).docx'\t\t   'Samuel Remington (pt. II).docx'\n",
            "'Crystal Powers.docx'\t\t\t   'Sandra Grayson.docx'\n",
            "'Crystal Powers (pt.II).docx'\t\t   'Sandra Grayson (pt. II).docx'\n",
            "'Duke Tad Rees.docx'\t\t\t   'Serenity Springs Map'\n",
            "'Duke Tad Rees (pt. II).docx'\t\t   'Sloane Sharpe.docx'\n",
            "'Harper Hayes.docx'\t\t\t   'Sloane Sharpe (pt. II).docx'\n",
            "'Harper Hayes (pt. II).docx'\t\t   'Sophia Astor.docx'\n",
            "'Jamie Blackwood.docx'\t\t\t   'Sophia Astor (pt. II).docx'\n",
            "'Jamie Blackwood (pt. II).docx'\t\t   'Suspect Guide - Eat, Pray, Die!.docx'\n",
            "'Miss Meera.docx'\t\t\t   'Travis Astor-Malone_.docx'\n",
            "'Miss Meera (pt. II).docx'\t\t   'Travis Astor-Malone (pt. II).docx'\n",
            "'Oliver Aldridge.docx'\t\t\t   'Ty Park.docx'\n",
            "'Oliver Aldridge (pt. II).docx'\t\t   'Ty Park (pt. II).docx'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vector DB"
      ],
      "metadata": {
        "id": "xRzQ5RQkZV2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install docx2txt\n",
        "\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "from llama_index.llms.groq import Groq\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "groq_llama = Groq(model=\"llama3-70b-8192\", api_key=groq_api_key)\n",
        "groq_gemma = Groq(model=\"gemma-7b-it\", api_key=groq_api_key)\n",
        "\n",
        "documents = SimpleDirectoryReader(\"./\").load_data(\"*.*\")\n",
        "\n",
        "llama_index = VectorStoreIndex.from_documents(documents, llm=groq_llama)\n",
        "llama_query_engine = llama_index.as_chat_engine()\n",
        "\n",
        "gemma_index = VectorStoreIndex.from_documents(documents, llm=groq_gemma)\n",
        "gemma_query_engine = gemma_index.as_chat_engine()\n",
        "\n",
        "openai_index = VectorStoreIndex.from_documents(documents)\n",
        "openai_query_engine = openai_index.as_chat_engine()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01gF_lL0Mhw3",
        "outputId": "1c37841d-02a6-4a97-d726-049b82adcff0"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: docx2txt in /usr/local/lib/python3.10/dist-packages (0.8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading files: 100%|██████████| 33/33 [00:00<00:00, 285.21file/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Questions"
      ],
      "metadata": {
        "id": "bO2xucRFN0lT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare response for simple questions\n",
        "\n",
        "response_llama_1 = llama_query_engine.query(\"Tell me what Colt Remington was doing during the murders.\")\n",
        "response_gemma_1 = gemma_query_engine.query(\"Tell me what Colt Remington was doing during the murders.\")\n",
        "response_openai_1 = openai_query_engine.query(\"Tell me what Colt Remington was doing during the murders.\")\n",
        "differences = openai_query_engine.query(f\"Tell me how different the responses are. Response 1 {response_llama_1} Response 2 {response_gemma_1} Response 3 {response_openai_1}. Be concise\")\n",
        "\n",
        "print(\"llama3 response:\")\n",
        "wrap_print(str(response_llama_1))\n",
        "\n",
        "print() # Add a line break here\n",
        "\n",
        "print(\"gemma response:\")\n",
        "wrap_print(str(response_gemma_1))\n",
        "\n",
        "print() # Add a line break here\n",
        "\n",
        "print(\"gpt-4o-mini response:\")\n",
        "wrap_print(str(response_openai_1))\n",
        "\n",
        "print() # Add a line break here\n",
        "\n",
        "print(\"Response comparison:\")\n",
        "wrap_print(str(differences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkURjtaKOBm1",
        "outputId": "b4bf8622-9ff8-42a1-e43e-89a3c7b4f4f7"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "llama3 response:\n",
            "Colt Remington was in the Main Hall sipping tea with Sloane during the\n",
            "murders.\n",
            "\n",
            "gemma response:\n",
            "Colt Remington was in the Main Hall sipping tea with Sloane during the\n",
            "murders.\n",
            "\n",
            "gpt-4o-mini response:\n",
            "Colt Remington was in the Main Hall sipping tea with Sloane during the\n",
            "murders.\n",
            "\n",
            "Response comparison:\n",
            "The responses are very similar. They all confirm that Colt Remington\n",
            "was in the Main Hall sipping tea with Sloane during the time of the\n",
            "murders.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare response for slightly more difficult questions\n",
        "response_llama_2 = llama_query_engine.query(\"What were some of Sloane Sharpe's motivations to commit the murders? Are her motivations strong compared to some of the other suspects?\")\n",
        "response_gemma_2 = gemma_query_engine.query(\"What were some of Sloane Sharpe's motivations to commit the murders? Are her motivations strong compared to some of the other suspects?\")\n",
        "response_openai_2 = openai_query_engine.query(\"What were some of Sloane Sharpe's motivations to commit the murders? Are her motivations strong compared to some of the other suspects?\")\n",
        "differences = openai_query_engine.query(f\"Tell me how different the responses are. Response 1 {response_llama_2} Response 2 {response_gemma_2} Response 3 {response_openai_2}. Be concise\")\n",
        "\n",
        "print(\"llama3 response:\")\n",
        "wrap_print(str(response_llama_2))\n",
        "\n",
        "print() # Add a line break here\n",
        "\n",
        "print(\"gemma response:\")\n",
        "wrap_print(str(response_gemma_2))\n",
        "\n",
        "print() # Add a line break here\n",
        "\n",
        "print(\"gpt-4o-mini response:\")\n",
        "wrap_print(str(response_openai_2))\n",
        "\n",
        "print() # Add a line break here\n",
        "\n",
        "print(\"Response comparison:\")\n",
        "wrap_print(str(differences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUK0jguUOt_P",
        "outputId": "c8b5c14e-7392-4893-97f0-a2ede271393b"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "llama3 response:\n",
            "Sloane Sharpe's motivations to commit the murders were driven by her\n",
            "desire to sabotage Crystal's fitness influencer status and to prevent\n",
            "Oliver Aldridge from returning to instructing full-time. These\n",
            "motivations seem strong compared to some of the other suspects.\n",
            "\n",
            "gemma response:\n",
            "Sloane Sharpe's motivations to commit the murders were driven by her\n",
            "desire to sabotage Crystal's fitness influencer status and to prevent\n",
            "Oliver Aldridge from returning to instructing full-time. Her\n",
            "motivations appear to be strong compared to other suspects.\n",
            "\n",
            "gpt-4o-mini response:\n",
            "Sloane Sharpe's motivations to commit the murders were driven by her\n",
            "desire to sabotage Crystal's fitness influencer status and prevent\n",
            "Oliver Aldridge from returning to instructing full-time. These\n",
            "motivations seem strong compared to some of the other suspects.\n",
            "\n",
            "Response comparison:\n",
            "The responses highlight that Sloane Sharpe's motivations to commit the\n",
            "murders were driven by her desire to sabotage Crystal's fitness\n",
            "influencer status and prevent Oliver Aldridge from returning to\n",
            "instructing full-time. These motivations appear particularly strong\n",
            "when compared to those of the other suspects involved in the case.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare other questions...\n",
        "\n",
        "response_llama_3 = llama_query_engine.query(\"Who was murdered? Where did the murders occur?\")\n",
        "response_gemma_3 = gemma_query_engine.query(\"Who was murdered? Where did the murders occur?\")\n",
        "response_openai_3 = openai_query_engine.query(\"Who was murdered? Where did the murders occur?\")\n",
        "differences = openai_query_engine.query(f\"Tell me how different the responses are. Response 1 {response_llama_3} Response 2 {response_gemma_3} Response 3 {response_openai_3}. Be concise\")\n",
        "\n",
        "print(\"llama3 response:\")\n",
        "wrap_print(str(response_llama_3))\n",
        "\n",
        "print() # Add a line break here\n",
        "\n",
        "print(\"gemma response:\")\n",
        "wrap_print(str(response_gemma_3))\n",
        "\n",
        "print() # Add a line break here\n",
        "\n",
        "print(\"gpt-4o-mini response:\")\n",
        "wrap_print(str(response_openai_3))\n",
        "\n",
        "print() # Add a line break here\n",
        "\n",
        "print(\"Response comparison:\")\n",
        "wrap_print(str(differences))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knZt-YPMSIAB",
        "outputId": "fc6237e1-ed75-465b-ed32-1fcc8fa5e29a"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "llama3 response:\n",
            "Oliver Aldridge was murdered. The murders occurred at Serenity\n",
            "Springs, specifically in the Studio where Crystal Powers was\n",
            "bludgeoned to death by a large jade statue of herself.\n",
            "\n",
            "gemma response:\n",
            "Oliver Aldridge was murdered. The murders occurred at Serenity\n",
            "Springs, specifically in the Studio where Crystal Powers was\n",
            "bludgeoned to death by a large jade statue of herself.\n",
            "\n",
            "gpt-4o-mini response:\n",
            "Oliver Aldridge was murdered. The murders occurred at Serenity\n",
            "Springs, specifically in the Studio where Crystal Powers was\n",
            "bludgeoned to death by a large jade statue of herself.\n",
            "\n",
            "Response comparison:\n",
            "The responses are different:  1. Oliver Aldridge was not murdered. The\n",
            "murder that occurred at Serenity Springs took place in the Studio\n",
            "where Crystal Powers was bludgeoned to death by a large jade statue of\n",
            "herself.  2. Harper Hayes and Jamie Blackwood claimed to be soaking in\n",
            "the hot springs near the Studio at the time of the murder,\n",
            "approximately 6:30pm. Harper had a motive to see Crystal Powers out of\n",
            "the picture, as she knew that Crystal's death would benefit Jamie in\n",
            "their business dealings. Additionally, Harper witnessed a heated\n",
            "argument between Crystal and Oliver Aldridge, which could have\n",
            "escalated to a point where Oliver became a threat to Crystal. Harper's\n",
            "proximity to the murder scene and her knowledge of the tensions\n",
            "between Crystal and Oliver make her a suspect in Oliver Aldridge's\n",
            "murder at Serenity Springs.  3. Oliver Aldridge was murdered at\n",
            "Serenity Springs in the Studio where Crystal Powers was also murdered.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Do the models generally understand what is happening?\n",
        "\n",
        "response_llama_4 = llama_query_engine.query(\"Can you describe what Eat, Pray, Die is?\")\n",
        "response_gemma_4 = gemma_query_engine.query(\"Can you describe what Eat, Pray, Die is?\")\n",
        "response_openai_4 = openai_query_engine.query(\"Can you describe what Eat, Pray, Die is?\")\n",
        "differences = openai_query_engine.query(f\"Tell me how different the responses are. Response 1 {response_llama_4} Response 2 {response_gemma_4} Response 3 {response_openai_4}. Be concise\")\n",
        "\n",
        "print(\"llama3 response:\")\n",
        "wrap_print(str(response_llama_4))\n",
        "\n",
        "print() # Add a line break here\n",
        "\n",
        "print(\"gemma response:\")\n",
        "wrap_print(str(response_gemma_4))\n",
        "\n",
        "print() # Add a line break here\n",
        "\n",
        "print(\"gpt-4o-mini response:\")\n",
        "wrap_print(str(response_openai_4))\n",
        "\n",
        "print() # Add a line break here\n",
        "\n",
        "print(\"Response comparison:\")\n",
        "wrap_print(str(differences))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v--HywZBSxU6",
        "outputId": "5e0f7b31-7798-43f7-925e-541f8d1a32d6"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "llama3 response:\n",
            "Eat, Pray, Die is an interactive \"Mingle\" mystery party where\n",
            "participants move around, speaking with other Suspects and\n",
            "Investigators to piece together a murder mystery. The goal is to\n",
            "identify the killer by investigating motives, means, and\n",
            "opportunities, while also concealing one's own motives and deflecting\n",
            "suspicion. The party involves mingling, questioning, and casting\n",
            "accusations before the big reveal.\n",
            "\n",
            "gemma response:\n",
            "Eat, Pray, Die is an interactive \"Mingle\" mystery party where\n",
            "participants move around, speaking with other Suspects and\n",
            "Investigators to gather clues and solve a murder mystery. The party\n",
            "involves investigating the murder, identifying the killer, and\n",
            "concealing one's own motives while following specific rules. The\n",
            "format of the party encourages mingling, questioning, and interaction\n",
            "among participants to piece together the mystery.\n",
            "\n",
            "gpt-4o-mini response:\n",
            "Eat, Pray, Die is an interactive \"Mingle\" mystery party where\n",
            "participants move around, speaking with other Suspects and\n",
            "Investigators to gather clues and solve a murder mystery. The party\n",
            "involves investigating the murder, identifying the killer, and\n",
            "concealing one's own motives while following specific rules. The\n",
            "format encourages mingling, questioning, and gathering information to\n",
            "piece together the mystery.\n",
            "\n",
            "Response comparison:\n",
            "Response 1: Participants move around, speaking with other Suspects and\n",
            "Investigators to piece together a murder mystery, identifying the\n",
            "killer by investigating motives, means, and opportunities, while\n",
            "concealing their own motives and deflecting suspicion.  Response 2:\n",
            "Participants move around, speaking with other Suspects and\n",
            "Investigators to gather clues and solve a murder mystery, identifying\n",
            "the killer, and concealing their own motives while following specific\n",
            "rules, encouraging mingling, questioning, and interaction.  Response\n",
            "3: Participants move around, speaking with other Suspects and\n",
            "Investigators to gather clues and solve a murder mystery, identifying\n",
            "the killer, and concealing their own motives while following specific\n",
            "rules, encouraging mingling, questioning, and gathering information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Can they solve the murder?\n",
        "\n",
        "response_llama_5 = llama_query_engine.query(\"You are an expert sleuth and are tasked with uncovering who is behind the murders of Crystal Powers and Oliver Aldridge. You don't have all the details but based on what you are able to infer you must identify who the murder (or murderers) are and describe in detail their motivations for committing the murders. Feel free to be expansive in your explanation.\")\n",
        "response_gemma_5 = gemma_query_engine.query(\"You are an expert sleuth and are tasked with uncovering who is behind the murders of Crystal Powers and Oliver Aldridge. You don't have all the details but based on what you are able to infer you must identify who the murder (or murderers) are and describe in detail their motivations for committing the murders. Feel free to be expansive in your explanation.\")\n",
        "response_openai_5 = openai_query_engine.query(\"You are an expert sleuth and are tasked with uncovering who is behind the murders of Crystal Powers and Oliver Aldridge. You don't have all the details but based on what you are able to infer you must identify who the murder (or murderers) are and describe in detail their motivations for committing the murders. Feel free to be expansive in your explanation.\")\n",
        "differences = openai_query_engine.query(f\"Tell me how different the responses are. Response 1 {response_llama_5} Response 2 {response_gemma_5} Response 3 {response_openai_5}. Be concise\")\n",
        "\n",
        "print(\"llama3 response:\")\n",
        "wrap_print(str(response_llama_5))\n",
        "\n",
        "print() # Add a line break here\n",
        "\n",
        "print(\"gemma response:\")\n",
        "wrap_print(str(response_gemma_5))\n",
        "\n",
        "print() # Add a line break here\n",
        "\n",
        "print(\"gpt-4o-mini response:\")\n",
        "wrap_print(str(response_openai_5))\n",
        "\n",
        "print() # Add a line break here\n",
        "\n",
        "print(\"Response comparison:\")\n",
        "wrap_print(str(differences))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Twsm-XaTRZA",
        "outputId": "bddad520-491f-439e-bdac-495480da4edd"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "llama3 response:\n",
            "Based on the information gathered, Samuel Remington is identified as\n",
            "the individual behind the murders of Crystal Powers and Oliver\n",
            "Aldridge. His motivations for committing these heinous acts were\n",
            "fueled by his desire to escape and avoid being apprehended for his\n",
            "actions. Feeling the need to flee, Remington took extreme measures to\n",
            "ensure his getaway, including resorting to murder to cover his tracks\n",
            "and sow confusion among the other guests. His actions were driven by a\n",
            "sense of self-preservation and a willingness to go to any lengths to\n",
            "evade capture.\n",
            "\n",
            "gemma response:\n",
            "Based on the information gathered, Samuel Remington is identified as\n",
            "the individual behind the murders of Crystal Powers and Oliver\n",
            "Aldridge. His motivations for committing these heinous acts were\n",
            "primarily fueled by his desire to escape and evade capture for his\n",
            "actions. It appears that Samuel Remington was driven by a sense of\n",
            "self-preservation and a need to avoid facing the consequences of his\n",
            "crimes.  It is possible that Samuel Remington felt threatened by\n",
            "Crystal Powers and Oliver Aldridge in some way, leading him to resort\n",
            "to extreme measures to eliminate them. His actions may have been a\n",
            "result of fear, paranoia, or a sense of desperation to protect himself\n",
            "from being exposed.  The murders of Crystal Powers and Oliver Aldridge\n",
            "could also be linked to a deeper, more complex motive that is yet to\n",
            "be fully uncovered. It is essential to delve further into Samuel\n",
            "Remington's background, relationships, and any potential conflicts or\n",
            "grievances that could have contributed to his decision to commit these\n",
            "murders.  In conclusion, Samuel Remington's motivations for the\n",
            "murders of Crystal Powers and Oliver Aldridge appear to be rooted in a\n",
            "combination of self-preservation, fear, and a desire to avoid\n",
            "accountability. Further investigation is necessary to unravel the full\n",
            "extent of his motives and the circumstances surrounding these tragic\n",
            "events.\n",
            "\n",
            "gpt-4o-mini response:\n",
            "Samuel Remington is the mastermind behind the murders of Crystal\n",
            "Powers and Oliver Aldridge. His motivations for committing these\n",
            "heinous acts were fueled by his desperate need to escape and avoid\n",
            "being caught for his actions. Remington felt the pressure closing in\n",
            "on him and decided to take drastic measures to ensure his getaway.\n",
            "To create chaos and divert attention from himself, Remington\n",
            "orchestrated the murders of Crystal Powers and Oliver Aldridge. By\n",
            "eliminating them, he hoped to sow confusion among the other guests and\n",
            "investigators, making it harder for them to piece together the truth.\n",
            "Remington's cunning and calculated approach to the murders reflect his\n",
            "determination to evade capture at all costs. His cold and calculated\n",
            "actions reveal a ruthless individual willing to go to extreme lengths\n",
            "to protect himself and his secrets.   As the expert sleuth on the\n",
            "case, it is crucial to unravel Remington's web of deception and bring\n",
            "him to justice for the murders of Crystal Powers and Oliver Aldridge.\n",
            "\n",
            "Response comparison:\n",
            "The responses provide different perspectives on Samuel Remington's\n",
            "motivations for the murders of Crystal Powers and Oliver Aldridge:  1.\n",
            "Response 1: Samuel Remington's actions were driven by a desire to\n",
            "escape and avoid apprehension. He took extreme measures, including\n",
            "murder, to ensure his getaway and sow confusion among others.  2.\n",
            "Response 2: Remington's motivations were primarily to escape and evade\n",
            "capture, driven by self-preservation and a need to avoid consequences.\n",
            "There may have been underlying feelings of threat or desperation\n",
            "leading to the murders.  3. Response 3: Remington orchestrated the\n",
            "murders to divert attention and escape capture. His calculated actions\n",
            "reflect a ruthless individual determined to protect himself at all\n",
            "costs.  Each response emphasizes different aspects of Remington's\n",
            "motives, ranging from self-preservation to creating chaos and avoiding\n",
            "accountability.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9WNIQM0cQ_bP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ua0N6LhrTrBS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}